# AWS機械学習関連用語解説

## AWS Glue DataBrew
コードを書かずにデータを準備するためのビジュアルデータ準備ツールです。データクレンジングや変換を簡単に行えます。機械学習モデルの前処理ステップを効率化し、データ品質を向上させます。

**主な特徴:**
- 250以上の組み込み変換機能
- ドラッグ＆ドロップのビジュアルインターフェース
- データプロファイリングと品質統計
- レシピベースのデータ変換
- スケジュール実行とジョブ管理
- データリネージの追跡

**サポートされるデータ変換:**
- データクレンジング（欠損値処理、外れ値検出など）
- データ型変換と標準化
- 集計と結合
- 特徴エンジニアリング（ビン分割、正規化など）
- テキストと日付の処理
- カスタム変換式

**ユースケース:**
- ML モデルのための特徴エンジニアリング
- データ品質の評価と改善
- ETL パイプラインの簡素化
- ビジネスアナリストによるセルフサービスデータ準備
- データサイエンティストの生産性向上
- データカタログの拡充と標準化

## Amazon SageMaker Data Wrangler
機械学習のためのデータ準備を簡素化するツールで、データのインポート、変換、分析、エクスポートを視覚的に行えます。エンドツーエンドのデータ準備ワークフローを SageMaker Studio 内で完結できます。

**主な特徴:**
- 300以上の組み込み変換機能
- データ可視化と探索的分析
- 自動データ品質と異常検出
- カスタム Python/PySpark 変換
- データフロー図によるプロセス管理
- SageMaker パイプラインとの統合

**データ分析機能:**
- 相関分析とターゲット漏洩検出
- 特徴重要度の評価
- 異常値と欠損値の分析
- データ分布の可視化
- バイアス検出と公平性分析
- 時系列データの分析

**ユースケース:**
- 機械学習モデルのための特徴エンジニアリング
- データ品質の問題の早期発見と修正
- 再利用可能なデータ準備パイプラインの構築
- 複数のデータソースの統合と変換
- データサイエンティストとデータエンジニアの協業
- モデル開発サイクルの短縮

## 破損画像変換（corrupt image）
画像データの品質問題を検出・修正するための変換処理です。機械学習モデルの堅牢性を向上させます。コンピュータビジョンモデルのトレーニングデータの品質を確保するための重要なステップです。

**主な問題と対処法:**
- 破損ファイル：検出と除外または修復
- 不完全な画像：部分的な復元または補間
- 低解像度：超解像技術による拡大
- ノイズ：フィルタリングとデノイジング
- 色調の問題：色補正とバランス調整
- 歪みや回転：幾何学的変換による修正

**実装アプローチ:**
- SageMaker Processing ジョブによる大規模処理
- AWS Lambda による軽量処理
- OpenCV や PIL などのライブラリの活用
- データ拡張による堅牢性の向上
- 自動検出と修正のパイプライン構築

**ユースケース:**
- 画像分類モデルのデータ品質向上
- 物体検出の精度向上
- 医療画像分析の信頼性確保
- 衛星画像や航空写真の前処理
- 監視カメラ映像の品質改善
- ユーザー生成コンテンツの品質管理

## Amazon SageMaker マネージドウォームプール
事前に初期化されたインスタンスのプールを維持し、トレーニングジョブの開始時間を短縮する機能です。インスタンスの起動と初期化にかかる時間を削減し、機械学習の反復サイクルを高速化します。

**主な特徴:**
- インスタンス起動時間の大幅短縮（最大 90%）
- 自動的なプールサイズ管理
- インスタンスタイプごとの設定
- キープアライブ期間のカスタマイズ
- コスト最適化のための自動スケーリング
- マルチアカウントでの共有オプション

**設定パラメータ:**
- インスタンスタイプとプールサイズ
- キープアライブ期間（分単位）
- リソース使用率のしきい値
- スケーリングポリシー
- タグとリソース割り当て

**ユースケース:**
- 反復的なモデル開発と実験
- ハイパーパラメータ最適化の高速化
- 本番環境での迅速なモデル更新
- バッチ推論ジョブの高速起動
- 分散トレーニングの効率化
- デモやワークショップでの応答性向上

## SageMaker 分散データ並列処理（SMDDP）ライブラリ
大規模モデルのトレーニングを複数のGPUインスタンスに効率的に分散するためのライブラリです。トレーニング時間を短縮します。通信オーバーヘッドを最小化し、GPU 利用率を最大化します。

**主な特徴:**
- AllReduce 操作の最適化
- AWS ネットワークインフラストラクチャの活用
- 自動的なグラデーション圧縮
- 適応的なオールリデュースアルゴリズム
- TensorFlow と PyTorch のネイティブサポート
- EFA（Elastic Fabric Adapter）との統合

**パフォーマンス最適化:**
- 通信とコンピューテーションのオーバーラップ
- 勾配の圧縮と量子化
- 最適化された集合通信アルゴリズム
- メモリ使用量の最適化
- ノード間帯域幅の効率的な利用

**ユースケース:**
- 大規模言語モデル（LLM）のトレーニング
- コンピュータビジョンモデルの分散トレーニング
- マルチノード GPU クラスターでのトレーニング
- トレーニング時間の短縮によるコスト最適化
- 数十億パラメータを持つモデルのトレーニング
- 研究開発サイクルの高速化

**サポートされるインスタンスタイプ:**
- P3、P4d、P5 インスタンス
- G4dn、G5 インスタンス
- Trn1 インスタンス（AWS Trainium）
- EFA 対応インスタンス
